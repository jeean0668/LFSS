import os
import subprocess
import streamlit as st
import faiss
import pickle
import ollama
from sentence_transformers import SentenceTransformer
import sys
from langchain.text_splitter import RecursiveCharacterTextSplitter
from utils.loader.FileLoader import FileLoader
import numpy as np

# âœ… Load document embedding model
embedding_model = SentenceTransformer("all-mpnet-base-v2")

def create_database(target_folder):
    """Function to create a FAISS database"""
    if not target_folder or not os.path.exists(target_folder):
        st.error("âŒ Please enter a valid folder.")
        return

    try:
        print(f"ğŸ“‚ Target Folder: {target_folder}")

        # âœ… Load documents (supports PDF, TXT, DOCX)
        file_loader = FileLoader(target_folder, show_progress=True)
        documents = file_loader.load_all_files()
        print(f"ğŸ” Loaded Document Count: {len(documents)}")

        # âœ… Preprocess documents
        def preprocess_text(text):
            # Example: convert to lowercase, remove stopwords, etc.
            text = text.lower()  # Convert to lowercase
            # Additional steps like stopword removal, tokenization can be added
            return text

        # âœ… Split documents (supports long documents) + save file paths
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

        docs = []
        metadata = []  # Save file paths
        original_texts = []  # Save original document content

        for doc in documents:
            try:
                # Apply preprocessing
                doc.page_content = preprocess_text(doc.page_content)
                split_docs = text_splitter.split_documents([doc])
                docs.extend(split_docs)
                metadata.extend([os.path.abspath(doc.metadata["source"])] * len(split_docs))  # Save full file paths
                original_texts.extend([doc.page_content] * len(split_docs))  # Save original content
            except Exception as e:
                print(f"âš ï¸ Error processing file, skipping: {doc.metadata['source']}, Error: {e}")

        if not docs:
            print("âŒ No documents found.")
            sys.exit(1)

        # âœ… Load document embedding model
        # embedding_model = SentenceTransformer("nomic-ai/nomic-embed-text-v1", trust_remote_code=True)
        # embedding_model = SentenceTransformer("distilbert-base-nli-mean-tokens")
        # embedding_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
        # embedding_model = SentenceTransformer("dunzhang/stella_en_400M_v5", trust_remote_code=True)
        # embedding_model = SentenceTransformer("jxm/cde-small-v2", trust_remote_code=True, device="cpu")
        embedding_model = SentenceTransformer("all-mpnet-base-v2")

        # âœ… Convert documents to vectors
        texts = [doc.page_content for doc in docs]
        vectors = embedding_model.encode(texts, show_progress_bar=True, convert_to_numpy=True)
        print(f"ğŸ” Number of converted vectors: {len(vectors)}")

        # âœ… Create FAISS index
        dimension = vectors.shape[1]
        index = faiss.IndexFlatL2(dimension)
        index.add(vectors)

        # âœ… Save FAISS index
        if not os.path.exists("db"):
            os.makedirs("db")
        
        faiss.write_index(index, "db/faiss_index.idx")
        # âœ… Save document mapping information (including full file paths and original content)
        with open("db/faiss_texts.pkl", "wb") as f:
            pickle.dump({"texts": texts, "metadata": metadata, "original_texts": original_texts}, f)

        st.info(f"ğŸ”„ Creating database... ğŸ“‚ Target folder: {target_folder}")
        st.success("âœ… Database creation complete!")

    except Exception as e:
        st.error("âŒ Database creation failed!")
        st.text_area("Error log", str(e))
        print(f"Error occurred: {e}")

def search_faiss(query, top_k=1):
    """Search FAISS and return related documents"""
    index = faiss.read_index("db/faiss_index.idx")

    with open("db/faiss_texts.pkl", "rb") as f:
        data = pickle.load(f)
        metadata = data["metadata"]
        original_texts = data["original_texts"]

    # LLMì„ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ê°€ê³µ
    system_prompt = """
    ì‚¬ìš©ìì˜ ê²€ìƒ‰ì–´ë¥¼ ë¶„ì„í•˜ì—¬ ë” ìì„¸í•˜ê³  êµ¬ì²´ì ì¸ ê²€ìƒ‰ì–´ë¡œ í™•ì¥í•´ì£¼ì„¸ìš”.
    ê²€ìƒ‰ì–´ë¥¼ í™•ì¥í•  ë•ŒëŠ” ì›ë˜ ê²€ìƒ‰ì–´ì˜ ì˜ë¯¸ë¥¼ ìœ ì§€í•˜ë©´ì„œ, ê´€ë ¨ëœ í‚¤ì›Œë“œë‚˜ ë™ì˜ì–´ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.
    ë‹µë³€ì€ í™•ì¥ëœ ê²€ìƒ‰ì–´ë§Œ ì œì‹œí•´ì£¼ì„¸ìš”.
    í™•ì¥ëœ ê²€ìƒ‰ì–´ëŠ” ì•„ë˜ í˜•íƒœë¡œ ì œì‹œí•´ ì£¼ì„¸ìš”.
    [í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3]
    í™•ì¥ëœ ê²€ìƒ‰ì–´ëŠ” ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ ì œì‹œí•´ ì£¼ì„¸ìš”.
    """
    
    llm_response = ollama.chat(
        model="gemma2:2b",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"ê²€ìƒ‰ì–´: {query}"}
        ]
    )
    
    # LLMì´ ê°€ê³µí•œ ê²€ìƒ‰ì–´ ì‚¬ìš©
    enhanced_query = llm_response["message"]["content"]
    print(f"ì›ë³¸ ê²€ìƒ‰ì–´: {query}")
    print(f"í™•ì¥ëœ ê²€ìƒ‰ì–´: {enhanced_query}")
    # query_vector = embedding_model.encode([query], convert_to_numpy=True)
    # ë¬¸ìì—´ì—ì„œ ëŒ€ê´„í˜¸ ì•ˆì˜ ë‚´ìš©ë§Œ ì¶”ì¶œí•˜ê³  ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    import re
    
    # ëŒ€ê´„í˜¸ ì•ˆì˜ ë‚´ìš©ë§Œ ì¶”ì¶œ
    match = re.search(r'\[(.*?)\]', enhanced_query)
    if match:
        # ëŒ€ê´„í˜¸ ì•ˆì˜ ë‚´ìš©ì„ ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ê³  ê° í•­ëª©ì˜ ê³µë°± ì œê±°
        keywords = [keyword.strip() for keyword in match.group(1).split(',')]
        # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ í•©ì¹˜ê¸°
        enhanced_query = ' '.join(keywords)
    else:
        # ëŒ€ê´„í˜¸ í˜•ì‹ì´ ì•„ë‹Œ ê²½ìš° ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
        enhanced_query = query
        
    # ê° í‚¤ì›Œë“œë³„ë¡œ ë²¡í„° ìƒì„± ë° ê²€ìƒ‰ ìˆ˜í–‰
    all_distances = []
    all_indices = []
    
    for keyword in keywords:
        query_vector = embedding_model.encode([keyword], convert_to_numpy=True)
        distances, indices = index.search(query_vector, top_k)
        all_distances.extend(distances[0])
        all_indices.extend(indices[0])
    
    # ë¦¬ìŠ¤íŠ¸ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
    distances = np.array([all_distances])
    indices = np.array([all_indices])

    results = sorted([
        {
            "file_name": os.path.basename(metadata[idx]),
            "full_path": metadata[idx], 
            "score": round(float(distances[0][i]), 4),
            "summary": original_texts[idx]
        }
        for i, idx in enumerate(indices[0])
        if idx != -1 and 0 <= idx < len(metadata)
    ], key=lambda x: x["score"], reverse=True)

    return results

def generate_llm_response(query, retrieved_docs):
    """Generate a response based on retrieved documents using Ollama LLM"""
    context = "\n\n".join([f"[{doc['file_name']}], {doc['summary']}" for doc in retrieved_docs])

    prompt = f"""
    User's question: {query}
    """
    
    system_prompt = f"""
    You are a smart local file explorer. Based on the question, you should recommend appropriate files and explain why. Information about the files is provided in the form of context.
    Here is the information found in the related documents:
    {context}
    The answer must be in Korean.
    """

    # response = ollama.chat(model="deepseek-r1:8b", messages=[{"role": "user", "content": prompt}])
    response = ollama.chat(model="gemma2:2b", messages=[{"role": "system", "content" : system_prompt}, {"role": "user", "content": prompt}])
    return response["message"]["content"]

# ğŸ“Œ Streamlit UI
st.title("ğŸ“„ Local File Semantic Search + LLM Response")

# âœ… Execute "Create Database" after user selects a folder
target_folder = st.text_input("ğŸ“‚ Enter target folder (supports PDF, TXT, DOCX):")

if st.button("ğŸ› ï¸ Create Database"):
    create_database(target_folder)

# âœ… Search UI
query = st.text_input("ğŸ” Enter search query:")

if query:
    with st.spinner("Searching..."):
        retrieved_docs = search_faiss(query)

        # ğŸ” Display search results
        st.subheader("ğŸ” List of Retrieved Documents:")
        for doc in retrieved_docs:
            st.markdown(f"**ğŸ“„ File Name:** {doc['file_name']}")
            st.markdown(f"ğŸ“‚ **Path:** `{doc['full_path']}`")
            st.markdown(f"ğŸ“œ **Similarity:** {doc['score']}...\n")
            st.markdown(f"ğŸ“‚ **Content Summary:** `{doc['summary'][:300]}`")
            
            st.markdown("---")

        # ğŸ“ Display LLM response
        st.subheader("ğŸ“ LLM Answer:")
        llm_response = generate_llm_response(query, retrieved_docs)
        st.write(llm_response)