import os
import subprocess
import streamlit as st
import faiss
import pickle
import ollama
from sentence_transformers import SentenceTransformer
import sys
from langchain.text_splitter import RecursiveCharacterTextSplitter
from utils.loader.FileLoader import FileLoader

# âœ… ë¬¸ì„œ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
embedding_model = SentenceTransformer("all-mpnet-base-v2")

def create_database(target_folder):
    """FAISS ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    if not target_folder or not os.path.exists(target_folder):
        st.error("âŒ ì˜¬ë°”ë¥¸ í´ë”ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        return

    try:
        print(f"ğŸ“‚ ëŒ€ìƒ í´ë”: {target_folder}")

        # âœ… ë¬¸ì„œ ë¡œë“œ (PDF, TXT, DOCX ì§€ì›)
        file_loader = FileLoader(target_folder, show_progress=True)
        documents = file_loader.load_all_files()
        print(f"ğŸ” ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")

        # âœ… ë¬¸ì„œ ì „ì²˜ë¦¬
        def preprocess_text(text):
            # ì˜ˆì‹œ: ì†Œë¬¸ì ë³€í™˜, ë¶ˆìš©ì–´ ì œê±° ë“±
            text = text.lower()  # ì†Œë¬¸ì ë³€í™˜
            # ë¶ˆìš©ì–´ ì œê±°, í† í°í™” ë“± ì¶”ê°€ ê°€ëŠ¥
            return text

        # âœ… ë¬¸ì„œ ë¶„í•  (ê¸´ ë¬¸ì„œ ì§€ì›) + íŒŒì¼ ê²½ë¡œ ì €ì¥
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=200)

        docs = []
        metadata = []  # íŒŒì¼ ê²½ë¡œ ì €ì¥
        original_texts = []  # ë¬¸ì„œ ì›ë³¸ ë‚´ìš© ì €ì¥

        for doc in documents:
            try:
                # ì „ì²˜ë¦¬ ì ìš©
                doc.page_content = preprocess_text(doc.page_content)
                split_docs = text_splitter.split_documents([doc])
                docs.extend(split_docs)
                metadata.extend([os.path.abspath(doc.metadata["source"])] * len(split_docs))  # íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œ ì €ì¥
                original_texts.extend([doc.page_content] * len(split_docs))  # ì›ë³¸ ë‚´ìš© ì €ì¥
            except Exception as e:
                print(f"âš ï¸ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ, ê±´ë„ˆëœ€: {doc.metadata['source']}, ì—ëŸ¬: {e}")

        if not docs:
            print("âŒ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)

        # âœ… ë¬¸ì„œ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        # embedding_model = SentenceTransformer("nomic-ai/nomic-embed-text-v1", trust_remote_code=True)
        # embedding_model = SentenceTransformer("distilbert-base-nli-mean-tokens")
        # embedding_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
        # embedding_model = SentenceTransformer("dunzhang/stella_en_400M_v5", trust_remote_code=True)
        # embedding_model = SentenceTransformer("jxm/cde-small-v2", trust_remote_code=True, device="cpu")
        embedding_model = SentenceTransformer("all-mpnet-base-v2")

        # âœ… ë¬¸ì„œ ë²¡í„° ë³€í™˜
        texts = [doc.page_content for doc in docs]
        vectors = embedding_model.encode(texts, show_progress_bar=True, convert_to_numpy=True)
        print(f"ğŸ” ë³€í™˜ëœ ë²¡í„° ìˆ˜: {len(vectors)}")

        # âœ… FAISS ì¸ë±ìŠ¤ ìƒì„±
        dimension = vectors.shape[1]
        index = faiss.IndexFlatL2(dimension)
        index.add(vectors)

        # âœ… FAISS ì¸ë±ìŠ¤ ì €ì¥
        
        if not os.path.exists("db"):
            os.makedirs("db")
        
        faiss.write_index(index, "db/faiss_index.idx")
        print("faiss checking point : ")
        # âœ… ë¬¸ì„œ ë§¤í•‘ ì •ë³´ ì €ì¥ (íŒŒì¼ ì „ì²´ ê²½ë¡œ + ì›ë³¸ ë‚´ìš© í¬í•¨)
        with open("db/faiss_texts.pkl", "wb") as f:
            pickle.dump({"texts": texts, "metadata": metadata, "original_texts": original_texts}, f)

        st.info(f"ğŸ”„ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„± ì¤‘... ğŸ“‚ ëŒ€ìƒ í´ë”: {target_folder}")
        st.success("âœ… ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ!")

    except Exception as e:
        st.error("âŒ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨!")
        st.text_area("ì—ëŸ¬ ë¡œê·¸", str(e))
        print(f"ì—ëŸ¬ ë°œìƒ: {e}")

def search_faiss(query, top_k=5):
    """FAISSì—ì„œ ê²€ìƒ‰í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ë°˜í™˜"""
    index = faiss.read_index("db/faiss_index.idx")

    with open("db/faiss_texts.pkl", "rb") as f:
        data = pickle.load(f)
        metadata = data["metadata"]
        original_texts = data["original_texts"]

    query_vector = embedding_model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_vector, top_k)

    results = sorted([
        {
            "file_name": os.path.basename(metadata[idx]),
            "full_path": metadata[idx], 
            "score": round(float(distances[0][i]), 4),
            "summary": original_texts[idx]
        }
        for i, idx in enumerate(indices[0])
        if idx != -1 and 0 <= idx < len(metadata)
    ], key=lambda x: x["score"], reverse=True)

    return results

def generate_llm_response(query, retrieved_docs):
    """Ollama LLMì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
    context = "\n\n".join([f"[{doc['file_name']}], {doc['summary']}" for doc in retrieved_docs])

    prompt = f"""
    ì‚¬ìš©ìì˜ ì§ˆë¬¸: {query}
    """
    
    system_prompt = f"""
    ë‹¹ì‹ ì€ ë˜‘ë˜‘í•œ ë¡œì»¬ íŒŒì¼ íƒìƒ‰ê¸° ì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ, ì ì ˆí•œ íŒŒì¼ë“¤ì„ ì¶”ì²œí•´ì£¼ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•´ ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤. íŒŒì¼ì— ëŒ€í•œ ì •ë³´ëŠ” context í˜•íƒœë¡œ ì œê³µë©ë‹ˆë‹¤.
    ë‹¤ìŒì€ ê´€ë ¨ ë¬¸ì„œë“¤ì—ì„œ ì°¾ì€ ì •ë³´ì…ë‹ˆë‹¤:
    {context}
    ëŒ€ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ í•´ì•¼ í•©ë‹ˆë‹¤. 
    """

    # response = ollama.chat(model="deepseek-r1:8b", messages=[{"role": "user", "content": prompt}])
    response = ollama.chat(model="gemma2:2b", messages=[{"role": "system", "content" : system_prompt}, {"role": "user", "content": prompt}])
    return response["message"]["content"]

# ğŸ“Œ Streamlit UI
st.title("ğŸ“„ FAISS ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ + LLM ë‹µë³€")

# âœ… ì‚¬ìš©ìê°€ í´ë” ì„ íƒ í›„ "Create Database" ì‹¤í–‰
target_folder = st.text_input("ğŸ“‚ ëŒ€ìƒ í´ë” ì…ë ¥ (PDF, TXT, DOCX ì§€ì›):")

if st.button("ğŸ› ï¸ Create Database"):
    create_database(target_folder)

# âœ… ê²€ìƒ‰ UI
query = st.text_input("ğŸ” ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")

if query:
    with st.spinner("ê²€ìƒ‰ ì¤‘..."):
        retrieved_docs = search_faiss(query)

        # ğŸ” ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥
        st.subheader("ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ëª©ë¡:")
        for doc in retrieved_docs:
            st.markdown(f"**ğŸ“„ íŒŒì¼ëª…:** {doc['file_name']}")
            st.markdown(f"ğŸ“‚ **ê²½ë¡œ:** `{doc['full_path']}`")
            st.markdown(f"ğŸ“œ **ìœ ì‚¬ë„:** {doc['score']}...\n")
            st.markdown(f"ğŸ“‚ **ë‚´ìš© ìš”ì•½:** `{doc['summary'][:300]}`")
            
            st.markdown("---")

        # ğŸ“ LLM ì‘ë‹µ ì¶œë ¥
        st.subheader("ğŸ“ LLM ë‹µë³€:")
        llm_response = generate_llm_response(query, retrieved_docs)
        st.write(llm_response)
